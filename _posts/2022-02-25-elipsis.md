---
layout: post
title: 파이썬에서의 점 세 개(...)의 의미
tags: [Python]
image:
  path: /images/abstract-1.jpg
  feature: abstract-1.jpg
---

이번 글에서는 파이썬에서의 점 세 개(...)의 의미에 대해 알아보겠습니다.

## Ellipsis

...를 파이썬에서 입력하면 아래와 같이 출력됩니다.

`In[1]:`
```python
{% raw %}...{% endraw %}
```
`Out[1]:`




    Ellipsis



Ellipsis라고 불리는 이 점 세 개는 아래와 같은 경우에 사용됩니다.

### 아직 작성되지 않은 함수, 클래스의 표시

`In[2]:`
```python
{% raw %}def a():
    {% endraw %}
```
`Out[2]:`


      File "<ipython-input-2-909bc61f35b6>", line 2
        
        ^
    SyntaxError: unexpected EOF while parsing
    


위와 같이 함수를 작성하다가 말면, EOF 에러가 발생하면서 코딩이 되지 않습니다.

`In[3]:`
```python
{% raw %}def a():
    ...{% endraw %}
```

반면, elipsis를 사용하면, 정상적으로 코딩이된 것을 확인할 수 있습니다.

이는 함수뿐만이 아니라, 마찬가지로 클래스에서도 사용할 수 있으며, 자주 사용되는 `pass`와 동일한 기능을 한다고 보시면 됩니다.

`In[4]:`
```python
{% raw %}def a():
    pass{% endraw %}
```

### Numpy에서의 활용

Numpy에서는 elipsis가 dimension에 대한 선택(slicing)으로 활용되기도 합니다.

`In[5]:`
```python
{% raw %}import numpy as np
a = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
a.shape{% endraw %}
```
`Out[5]:`




    (2, 2, 2)



이 상태에서 ...을 이용하면, 지정한 dimension에 해당하는 모든 행열을 출력해줍니다.

`In[6]:`
```python
{% raw %}a[0, ...] # 첫번째 dimension=0인 모든 값 출력{% endraw %}
```
`Out[6]:`




    array([[1, 2],
           [3, 4]])



`In[7]:`
```python
{% raw %}a[..., 0] # 마지막 dimension=0인 모든 값 출력{% endraw %}
```
`Out[7]:`




    array([[1, 3],
           [5, 7]])



dimension을 굳이 지정하지 않는다면 모든 값을 출력해줍니다.

`In[8]:`
```python
{% raw %}a[...]{% endraw %}
```
`Out[8]:`




    array([[[1, 2],
            [3, 4]],
    
           [[5, 6],
            [7, 8]]])



다만, 아래와 같이 여러 elipsis를 한 번에 사용하는 것은 안됩니다.

`In[9]:`
```python
{% raw %}a[..., 0, ...]{% endraw %}
```
`Out[9]:`


    ---------------------------------------------------------------------------

    IndexError                                Traceback (most recent call last)

    <ipython-input-9-962c57d90f4e> in <module>
    ----> 1 a[..., 0, ...]
    

    IndexError: an index can only have a single ellipsis ('...')


이를 활용하면, array의 덮어쓰기 없이 값만 바꿔줄 수도 있습니다.

`In[10]:`
```python
{% raw %}a = np.array([[1, 2], [3, 4]])
b = np.array([[5, 6], [7, 8]]){% endraw %}
```

`In[11]:`
```python
{% raw %}print("a:", id(a), a)
print("b:", id(b), b){% endraw %}
```
`Out[11]:`

    a: 2218041591488 [[1 2]
     [3 4]]
    b: 2218176484096 [[5 6]
     [7 8]]
    

a와 b의 id는 각각 다르지만, `a=b`를 실행하면 a의 id가 b의 id로 바뀌어버립니다. 즉, 원래 a는 메모리에서 호출할 수 없게되게 됩니다. (이는 아래에 더 설명되어 있습니다.) 하지만, elipsis를 활용하면 값만 바꿀 수도 있습니다.

`In[12]:`
```python
{% raw %}a[...] = b[...]{% endraw %}
```

`In[13]:`
```python
{% raw %}print("a:", id(a), a)
print("b:", id(b), b){% endraw %}
```
`Out[13]:`

    a: 2218041591488 [[5 6]
     [7 8]]
    b: 2218176484096 [[5 6]
     [7 8]]
    

위처럼 a의 값만 변하고, id는 변하지 않았습니다. 반면 위에서 언급한 것과 같이 아래를 실행하면

`In[14]:`
```python
{% raw %}a = b{% endraw %}
```

`In[15]:`
```python
{% raw %}print("a:", id(a), a)
print("b:", id(b), b){% endraw %}
```
`Out[15]:`

    a: 2218176484096 [[5 6]
     [7 8]]
    b: 2218176484096 [[5 6]
     [7 8]]
    

a의 id마저 바뀐 것을 볼 수 있습니다.

## PyTorch에서의 활용

위와 같은 성질은 딥러닝 프레임워크인 PyTorch에서도 활용 가능합니다.

### 값 대체하기

`In[16]:`
```python
{% raw %}import torch
a = torch.rand(2,2)
b = torch.rand(2,2)
a,b{% endraw %}
```
`Out[16]:`




    (tensor([[3.4571e-06, 2.1676e-01],
             [3.0452e-01, 8.3729e-01]]),
     tensor([[0.0160, 0.8569],
             [0.9595, 0.8538]]))



`In[17]:`
```python
{% raw %}id(a), id(b){% endraw %}
```
`Out[17]:`




    (2218176497728, 2218176497984)



`In[18]:`
```python
{% raw %}a = b
id(a),id(b){% endraw %}
```
`Out[18]:`




    (2218176497984, 2218176497984)



`In[19]:`
```python
{% raw %}# elipsis는 id를 바꾸지 않음.
a[...] = b[...]
id(a),id(b){% endraw %}
```
`Out[19]:`




    (2218176497984, 2218176497984)



`In[20]:`
```python
{% raw %}# clone은 id를 바꿈.
a = b.clone()
id(a),id(b){% endraw %}
```
`Out[20]:`




    (2218176521408, 2218176497984)



### Gradient 조심하기

하지만, 값을 대체하는 것은 gradient까지 가지고 가는 것이기 때문에 아래 사항처럼 Gradient 관련 에러를 주의해야합니다.

`In[21]:`
```python
{% raw %}import torch
a = torch.rand(2,2)
b = torch.rand(2,2)
a,b{% endraw %}
```
`Out[21]:`




    (tensor([[0.7798, 0.3220],
             [0.0681, 0.4200]]),
     tensor([[0.3913, 0.3768],
             [0.6379, 0.4930]]))



`In[22]:`
```python
{% raw %}b.requires_grad = True{% endraw %}
```

`In[23]:`
```python
{% raw %}a[0,...] = b[0,...]
print(id(a), id(b))
print(a),print(b){% endraw %}
```
`Out[23]:`

    2218260990016 2218176521408
    tensor([[0.3913, 0.3768],
            [0.0681, 0.4200]], grad_fn=<CopySlices>)
    tensor([[0.3913, 0.3768],
            [0.6379, 0.4930]], requires_grad=True)
    




    (None, None)



`In[24]:`
```python
{% raw %}b.sum().backward(){% endraw %}
```

`In[25]:`
```python
{% raw %}b.grad{% endraw %}
```
`Out[25]:`




    tensor([[1., 1.],
            [1., 1.]])



`In[26]:`
```python
{% raw %}a.grad # a는 requires_grad=True인 b의 값을 전달받는 것이므로 leaf Tensor가 아님. 따라서 grad를 계산할 수 없음.{% endraw %}
```
`Out[26]:`

    <ipython-input-26-d40920619642>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.
      a.grad
    
